{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Environment settings: \n",
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    ##### Web scrapper for infinite scrolling page #####\n",
    "    driver = webdriver.Chrome(executable_path=r\"E:\\Chromedriver\\chromedriver_win32_chrome83\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 1\n",
    "\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if i == 10:\n",
    "            break \n",
    "\n",
    "    ##### Extract Reddit URLs #####\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://finance.yahoo.com' #Global Variable \n",
    "\n",
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find_all('span')[0].text #source\n",
    "    news_time = news_tag.find_all('span')[1].text #time\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_url = news_tag.find('a')['href'] #link\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'time' : news_time,    \n",
    "            'headline' : news_headline,\n",
    "            'url' : BASE_URL + news_url,\n",
    "            'content' : news_content,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    #print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    #print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    #print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    #print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    #news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://finance.yahoo.com/topic/stock-market-news/'\n",
    "# doc = get_page(url)\n",
    "# news_list = get_news_tags(doc)\n",
    "# news_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Stock Market News From Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# now = datetime.now()\n",
    "# current_time = now.strftime(\"%H:%M:%S\")\n",
    "# print(\"Report Date = \", date.today())\n",
    "# print(\"Report Time =\", current_time)\n",
    "# YAHOO_NEWS_URL = BASE_URL + '/topic/stock-market-news/'\n",
    "# news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis of Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# sentiment = [None] * len(news_df)\n",
    "# sentiment_score = [None] * len(news_df)\n",
    "# index = -1\n",
    "# for sentence in news_df['headline']:\n",
    "#     index+=1\n",
    "#     result = sentiment_pipeline(sentence[:512])[0]\n",
    "#     sentiment[index] = result['label']\n",
    "#     sentiment_score[index] = result['score']\n",
    "# news_df['Sentiment'] = sentiment\n",
    "# news_df['Score'] = sentiment_score\n",
    "\n",
    "\n",
    "# sentiment = [None] * len(news_df)\n",
    "# sentiment_score = [None] * len(news_df)\n",
    "# index = -1\n",
    "# for sentence in news_df['content']:\n",
    "#     index+=1\n",
    "#     result = sentiment_pipeline(sentence[:512])[0]\n",
    "#     sentiment[index] = result['label']\n",
    "#     sentiment_score[index] = result['score']\n",
    "# news_df['Sentiment2'] = sentiment\n",
    "# news_df['Score2'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df[news_df['Score'] > 0.8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(x = 'Sentiment', data = news_df[news_df['Score'] > 0.8], hue = 'Sentiment', hue_order=['POSITIVE', 'NEGATIVE']).set_title(\"Sentiment of Headline\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.histplot(x = 'Sentiment2', data = news_df[news_df['Score2'] > 0.8], hue = 'Sentiment2', hue_order=['POSITIVE', 'NEGATIVE']).set_title(\"Sentiment of Content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/Sentiment_SNP')\n",
    "\n",
    "import helper_functions\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each Articles - From Finviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import socket\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Report Date = \", date.today())\n",
    "print(\"Report Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping useful information\n",
    "\n",
    "url = \"https://finviz.com/news.ashx\"\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "cookies = {\"CONSENT\": \"YES+cb.20210720-07-p0.en+FX+410\"}\n",
    "\n",
    "req = Request(url, headers=headers)\n",
    "\n",
    "try:\n",
    "    contents = urlopen(req).read() \n",
    "    soup = BeautifulSoup(contents, features=\"html.parser\")\n",
    "\n",
    "    link2 = []\n",
    "    headline2 = []\n",
    "    date2 = []\n",
    "\n",
    "    for row in soup.find_all('tr', class_ = 'nn'):\n",
    "        headline = row.find('a', class_ = 'nn-tab-link')\n",
    "       \n",
    "        if headline == None:\n",
    "            continue\n",
    "\n",
    "        headline2.append(headline.text)\n",
    "        link = row.find('a', class_ = 'nn-tab-link')\n",
    "        link2.append(link['href'])\n",
    "        date = row.find('td', class_ = 'nn-date')\n",
    "        date2.append(date)\n",
    "\n",
    "except urllib.error.HTTPError as err:\n",
    "    print(err.code)\n",
    "\n",
    "except socket.timeout as se:\n",
    "    print(\"socket timeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning the data into a dataframe\n",
    "df = pd.DataFrame(date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df[0]\n",
    "df['Headline'] = headline2\n",
    "df = pd.DataFrame(list(zip(df['Date'], df['Headline'])), columns=['Date', 'Headline'])\n",
    "df['Links'] = link2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filtering only links starting with https\n",
    "# df['Links_True'] = list(\n",
    "#     map(lambda x: x.startswith('http'), df['Links']))\n",
    "\n",
    "# df = df[df['Links_True'] == True].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word count of headline (no longer needed)\n",
    "i = []\n",
    "for string in df['Headline']:\n",
    "    i.append(count_words(string))\n",
    "df['word_count'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis of headline\n",
    "df['Sentiment_Head'] = -2\n",
    "\n",
    "sentiment = [None] * len(df)\n",
    "sentiment_score = [None] * len(df)\n",
    "index = -1\n",
    "for sentence in df['Headline']:\n",
    "    index+=1\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "    sentiment_score[index] = result['score']\n",
    "df['Sentiment_Head'] = sentiment\n",
    "df['Score_Head'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect entire articles\n",
    "\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n",
    "cookies = {\"CONSENT\": \"YES+cb.20210720-07-p0.en+FX+410\"}\n",
    "\n",
    "article = []\n",
    "\n",
    "for url in df['Links']:\n",
    "    req = Request(url, headers=headers)\n",
    "\n",
    "    try:\n",
    "        contents = urlopen(req).read() \n",
    "        soup = BeautifulSoup(contents, features=\"html.parser\")\n",
    "\n",
    "        p = soup.find_all('p')\n",
    "        paragraphs = []\n",
    "        for x in p:\n",
    "            paragraphs.append(str(x.text))\n",
    "\n",
    "        paragraphs = ''.join(paragraphs)\n",
    "\n",
    "        article.append(paragraphs) \n",
    "\n",
    "    except urllib.error.HTTPError as err:\n",
    "        print(err.code)\n",
    "\n",
    "    except socket.timeout as se:\n",
    "        print(\"socket timeout\")\n",
    "\n",
    "df['article'] = article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Score_Head'] > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Articles\n",
    "summary_pipeline = pipeline(\"summarization\", model=\"pszemraj/long-t5-tglobal-base-16384-book-summary\")\n",
    "\n",
    "summary = [None] * len(df)\n",
    "index = -1\n",
    "for articles in df['article']:\n",
    "    index+=1\n",
    "    result = summary_pipeline(articles[:512])[0]\n",
    "    summary[index] = result\n",
    "df['summary'] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis of Article Summary\n",
    "df['Sentiment_Article'] = -2\n",
    "\n",
    "sentiment = [None] * len(df)\n",
    "sentiment_score = [None] * len(df)\n",
    "index = -1\n",
    "for sentence in df['Headline']:\n",
    "    index+=1\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "    sentiment_score[index] = result['score']\n",
    "df['Sentiment_Article'] = sentiment\n",
    "df['Score_Article'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].to_string()\n",
    "df.drop_duplicates(subset='summary', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the bar chart from 5 rated reviews ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df[\"Headline\"]:\n",
    "    for word in generate_ngrams(sent,2):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(10), 'green')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "#fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=600, width=600, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "998c911629ba937bcf1bf80465453e12e8c5c2c818cb936f93ef7cf495a937a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
