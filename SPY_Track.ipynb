{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Woon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from prophet import Prophet\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "import tkinter.messagebox as mbox\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "# Environment settings: \n",
    "pd.set_option('display.max_column', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_seq_items', None)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    ##### Web scrapper for infinite scrolling page #####\n",
    "    driver = webdriver.Chrome(executable_path=r\"E:\\Chromedriver\\chromedriver_win32_chrome83\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow 2 seconds for the web page to open\n",
    "    scroll_pause_time = 1 # You can set your own pause time. My laptop is a bit slow so I use 1 sec\n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")   # get the screen height of the web\n",
    "    i = 1\n",
    "\n",
    "    while True:\n",
    "        # scroll one screen height each time\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # update scroll height each time after scrolled, as the scroll height can change after we scrolled the page\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")  \n",
    "        # Break the loop when the height we need to scroll to is larger than the total scroll height\n",
    "        if i == 20:\n",
    "            break \n",
    "\n",
    "    ##### Extract Reddit URLs #####\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_tags(doc):\n",
    "    \"\"\"Get the list of tags containing news information\"\"\"\n",
    "    news_class = \"Ov(h) Pend(44px) Pstart(25px)\" ## class name of div tag \n",
    "    news_list  = doc.find_all('div', {'class': news_class})\n",
    "    return news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_news(news_tag):\n",
    "    \"\"\"Get the news data point and return dictionary\"\"\"\n",
    "    news_source = news_tag.find_all('span')[0].text #source\n",
    "    news_time = news_tag.find_all('span')[1].text #link\n",
    "    news_headline = news_tag.find('a').text #heading\n",
    "    news_content = news_tag.find('p').text #content\n",
    "    news_image = news_tag.findParent().find('img')['src'] #thumb image\n",
    "    return { 'source' : news_source,\n",
    "            'time' : news_time,    \n",
    "            'headline' : news_headline,\n",
    "            'content' : news_content,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_yahoo_news(url, path=None):\n",
    "    \"\"\"Get the yahoo finance market news and write them to CSV file \"\"\"\n",
    "    if path is None:\n",
    "        path = 'stock-market-news.csv'\n",
    "        \n",
    "    #print('Requesting html page')\n",
    "    doc = get_page(url)\n",
    "\n",
    "    #print('Extracting news tags')\n",
    "    news_list = get_news_tags(doc)\n",
    "\n",
    "    #print('Parsing news tags')\n",
    "    news_data = [parse_news(news_tag) for news_tag in news_list]\n",
    "\n",
    "    #print('Save the data to a CSV')\n",
    "    news_df = pd.DataFrame(news_data)\n",
    "    #news_df.to_csv(path, index=None)\n",
    "    \n",
    "    #This return statement is optional, we are doing this just analyze the final output \n",
    "    return news_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'C:/Users/Woon/Desktop/Columbia/Applied Analytics/Term3/Sentiment_SNP')\n",
    "\n",
    "import helper_functions\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.14.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from plotly import tools\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import socket\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_input = input(\"Type in the Ticker: \")\n",
    "# print(\"You entered:\", text_input)\n",
    "\n",
    "text_input = 'xom'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yahoo Finance News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Date =  2023-04-12\n",
      "Report Time = 23:25:05\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Report Date = \", date.today())\n",
    "print(\"Report Time =\", current_time)\n",
    "YAHOO_NEWS_URL = f'https://finance.yahoo.com/quote/{text_input}?p={text_input}&.tsrc=fin-srch'\n",
    "news_df = scrape_yahoo_news(YAHOO_NEWS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'headline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Woon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Woon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Woon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'headline'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Woon\\Desktop\\Columbia\\Applied Analytics\\Term3\\Sentiment_SNP\\SPY_Track.ipynb Cell 14\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Woon/Desktop/Columbia/Applied%20Analytics/Term3/Sentiment_SNP/SPY_Track.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sentiment_score \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(news_df)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Woon/Desktop/Columbia/Applied%20Analytics/Term3/Sentiment_SNP/SPY_Track.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Woon/Desktop/Columbia/Applied%20Analytics/Term3/Sentiment_SNP/SPY_Track.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m news_df[\u001b[39m'\u001b[39;49m\u001b[39mheadline\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Woon/Desktop/Columbia/Applied%20Analytics/Term3/Sentiment_SNP/SPY_Track.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     index\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Woon/Desktop/Columbia/Applied%20Analytics/Term3/Sentiment_SNP/SPY_Track.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     result \u001b[39m=\u001b[39m sentiment_pipeline(sentence[:\u001b[39m512\u001b[39m])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Woon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Woon\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'headline'"
     ]
    }
   ],
   "source": [
    "sentiment = [None] * len(news_df)\n",
    "sentiment_score = [None] * len(news_df)\n",
    "index = -1\n",
    "for sentence in news_df['headline']:\n",
    "    index+=1\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "    sentiment_score[index] = result['score']\n",
    "news_df['Sentiment'] = sentiment\n",
    "news_df['Score'] = sentiment_score\n",
    "\n",
    "\n",
    "sentiment = [None] * len(news_df)\n",
    "sentiment_score = [None] * len(news_df)\n",
    "index = -1\n",
    "for sentence in news_df['content']:\n",
    "    index+=1\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "    sentiment_score[index] = result['score']\n",
    "news_df['Sentiment2'] = sentiment\n",
    "news_df['Score2'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[news_df['Score'] > 0.8]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x = 'Sentiment', data = news_df[news_df['Score'] > 0.8], hue = 'Sentiment', hue_order=['POSITIVE', 'NEGATIVE'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x = 'Sentiment2', data = news_df[news_df['Score2'] > 0.8], hue = 'Sentiment2', hue_order=['POSITIVE', 'NEGATIVE'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the bar chart from 5 rated reviews ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in news_df[\"headline\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "#fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=600, width=600, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the bar chart from 5 rated reviews ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in news_df[\"content\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(20), 'green')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "#fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=600, width=600, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://finviz.com/quote.ashx?t={text_input}&ty=c&ta=1&p=d\"\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "cookies = {\"CONSENT\": \"YES+cb.20210720-07-p0.en+FX+410\"}\n",
    "\n",
    "req = Request(url, headers=headers)\n",
    "\n",
    "try:\n",
    "    contents = urlopen(req).read() \n",
    "    soup = BeautifulSoup(contents, features=\"html.parser\")\n",
    "\n",
    "    sentence2 = []\n",
    "\n",
    "    for tag in soup.find_all('a'):\n",
    "\n",
    "        sentence = tag.text.split(\".\")\n",
    "        sentence2.append(sentence)\n",
    "\n",
    "except urllib.error.HTTPError as err:\n",
    "    print(err.code)\n",
    "\n",
    "except socket.timeout as se:\n",
    "    print(\"socket timeout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headline'] = df[0]\n",
    "df['Sentiment'] = -2\n",
    "df = pd.DataFrame(list(zip(df['Headline'], df['Sentiment'])), columns=['Headline', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = []\n",
    "\n",
    "for string in df['Headline']:\n",
    "    i.append(count_words(string))\n",
    "\n",
    "df['word_count'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = [None] * len(df)\n",
    "sentiment_score = [None] * len(df)\n",
    "index = -1\n",
    "for sentence in df['Headline']:\n",
    "    index+=1\n",
    "    result = sentiment_pipeline(sentence[:512])[0]\n",
    "    sentiment[index] = result['label']\n",
    "    sentiment_score[index] = result['score']\n",
    "df['Sentiment'] = sentiment\n",
    "df['Score'] = sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['word_count'] > 10]\n",
    "df['Headline_Lower'] = df['Headline'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x = 'Sentiment', data = df[df['Score'] > 0.8], hue = 'Sentiment', hue_order=['POSITIVE', 'NEGATIVE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the bar chart from 5 rated reviews ##\n",
    "freq_dict = defaultdict(int)\n",
    "for sent in df[\"Headline_Lower\"]:\n",
    "    for word in generate_ngrams(sent,3):\n",
    "        freq_dict[word] += 1\n",
    "fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "trace0 = horizontal_bar_chart(fd_sorted.head(10), 'green')\n",
    "\n",
    "# Creating two subplots\n",
    "fig = tools.make_subplots(rows=1, cols=1, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words\"])\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "#fig.append_trace(trace1, 1, 2)\n",
    "fig['layout'].update(height=600, width=600, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "py.iplot(fig, filename='word-plots');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_ohlc_df = yf.download(text_input, start=dt.date.today() - dt.timedelta(days=1095), end=dt.date.today())\n",
    "df=spy_ohlc_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15,6)\n",
    "# sns.lineplot(x = 'Date', y = 'Close', data = df)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SPY price data\n",
    "spy_data = df\n",
    "spy_data = df[['Date', 'Close']]\n",
    "spy_data.columns = ['ds', 'y']\n",
    "\n",
    "# Initialize Prophet model\n",
    "model = Prophet()\n",
    "\n",
    "# Fit the model on SPY data\n",
    "model.fit(spy_data)\n",
    "\n",
    "# Create future dates for forecasting\n",
    "future_dates = model.make_future_dataframe(periods=3)\n",
    "\n",
    "# Make predictions for future dates\n",
    "forecast = model.predict(future_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = model.plot(forecast, xlabel='Date', ylabel='SPY Price')\n",
    "# fig.set_size_inches(15,6)\n",
    "# plt.title('Forecasted Price')\n",
    "# plt.xlabel('dt')\n",
    "# plt.ylabel('Price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecasted data\n",
    "model.plot(forecast, xlabel='Date', ylabel='SPY Price')\n",
    "plt.xlim(dt.date.today() - dt.timedelta(days=7), dt.date.today() + dt.timedelta(days=3))\n",
    "\n",
    "# Plot the forecasted trend and seasonality components\n",
    "fig = model.plot_components(forecast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_ohlc_df = yf.download(text_input, start=dt.date.today() - dt.timedelta(days=7), end=dt.date.today(), interval='1m')\n",
    "df1=spy_ohlc_df.reset_index()\n",
    "\n",
    "# spy_ohlc_df = yf.download(text_input, start=dt.date.today() - dt.timedelta(days=7), end=dt.date.today())\n",
    "# df1=spy_ohlc_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(15,6)\n",
    "# sns.lineplot(x = 'Datetime', y = 'Close', data = df1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SPY price data\n",
    "spy_data = df1\n",
    "spy_data = df1[['Datetime', 'Close']]\n",
    "#spy_data = df1[['Date', 'Close']]\n",
    "spy_data.columns = ['ds', 'y']\n",
    "\n",
    "# Initialize Prophet model\n",
    "model = Prophet()\n",
    "\n",
    "# Fit the model on SPY data\n",
    "model.fit(spy_data)\n",
    "\n",
    "# Create future dates for forecasting\n",
    "future_dates = model.make_future_dataframe(periods=2880, freq='T')\n",
    "\n",
    "# Make predictions for future dates\n",
    "forecast = model.predict(future_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = model.plot(forecast, xlabel='Date', ylabel='SPY Price')\n",
    "# fig.set_size_inches(15,6)\n",
    "# plt.title('Forecasted Price')\n",
    "# plt.xlabel('dt')\n",
    "# plt.ylabel('Price')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the forecasted data\n",
    "model.plot(forecast, xlabel='Date', ylabel='SPY Price')\n",
    "plt.xlim(dt.date.today() - dt.timedelta(days=1), dt.date.today() + dt.timedelta(minutes=1440))\n",
    "\n",
    "# Plot the forecasted trend and seasonality components\n",
    "fig = model.plot_components(forecast)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "998c911629ba937bcf1bf80465453e12e8c5c2c818cb936f93ef7cf495a937a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
